{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dog images are 8351 \n",
      "Total human images are 13233 \n"
     ]
    }
   ],
   "source": [
    "#load images\n",
    "dog_files = np.array(glob(\"dogImages/*/*/*\"))\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('Total dog images are %d ' % len(dog_files))\n",
    "print('Total human images are %d ' % len(human_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#face detector function\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img_path):\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(image_gray)\n",
    "    \n",
    "    return len(faces) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of human in human files is  100.0\n",
      "The percentage of dog in dog files is  52.0\n"
     ]
    }
   ],
   "source": [
    "#calculate percentage of human in both files\n",
    "\n",
    "human_files_per = human_files[:100]\n",
    "dog_files_per = dog_files[:100]\n",
    "\n",
    "sum_human_hd = 0\n",
    "sum_dog_hd = 0\n",
    "\n",
    "for i in range(0,len(human_files_per)):\n",
    "    human_hd = human_files_per[i]\n",
    "    dog_hd = dog_files_per[i]\n",
    "    \n",
    "    if face_detector(human_hd) == True:\n",
    "        sum_human_hd += 1\n",
    "    if face_detector(dog_hd) == True:\n",
    "        sum_dog_hd += 1\n",
    "\n",
    "per_human_hd = (sum_human_hd/len(human_files_per))*100\n",
    "per_dog_hd = (sum_dog_hd/len(dog_files_per))*100\n",
    "print(\"The percentage of human in human files is \" ,per_human_hd)\n",
    "print(\"The percentage of dog in dog files is \" ,per_dog_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of dog in both files\n",
    "#dog detetctor using vgg16 model\n",
    "\n",
    "VGG = models.vgg16(pretrained = True)\n",
    "\n",
    "#VGG function to return index of images from ImageNet\n",
    "def VGG_16(img):\n",
    "#def VGG16_predict(img):\n",
    "    image = Image.open(img)\n",
    "    data_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "    image_trans = data_transform(image)\n",
    "    imagee = image_trans.unsqueeze(0)\n",
    "    image_var = Variable(imagee)\n",
    "    image_vgg = VGG(image_var)\n",
    "    image_index = image_vgg.data.numpy().argmax()\n",
    "    \n",
    "    return image_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dog detector index b/w 151 and 268\n",
    "def dog_detector(img):\n",
    "    index_re = VGG_16(img)\n",
    "    \n",
    "    if index_re >= 151 and index_re <= 268:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of human in human files is  1.0\n",
      "The percentage of dog in dog files is  92.0\n"
     ]
    }
   ],
   "source": [
    "#calculate percentage of dog in both files\n",
    "human_files_per = human_files[:100]\n",
    "dog_files_per = dog_files[:100]\n",
    "\n",
    "sum_human_dd = 0\n",
    "sum_dog_dd = 0\n",
    "\n",
    "for i in range(0,len(human_files_per)):\n",
    "    human_dd = human_files_per[i]\n",
    "    dog_dd = dog_files_per[i]\n",
    "    \n",
    "   # d = dog_detector(dog_dd)\n",
    "    \n",
    "    if dog_detector(human_dd) == True:\n",
    "        sum_human_dd += 1\n",
    "    if dog_detector(dog_dd) == True:\n",
    "        sum_dog_dd += 1\n",
    "\n",
    "per_human_dd = (sum_human_dd/len(human_files_per))*100\n",
    "per_dog_dd = (sum_dog_dd/len(dog_files_per))*100\n",
    "print(\"The percentage of human in human files is \" ,per_human_dd)\n",
    "print(\"The percentage of dog in dog files is \" ,per_dog_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "133\n",
      "446\n",
      "56\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "#data preperation\n",
    "data_train = os.path.join(\"dogImages/\",\"train/\")\n",
    "data_valid = os.path.join(\"dogImages/\",\"valid/\")\n",
    "data_test = os.path.join(\"dogImages/\",\"test/\")\n",
    "\n",
    "#transforms\n",
    "\n",
    "data_transform = { \n",
    "                    'train' : transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])]),\n",
    "    \n",
    "                    'valid' : transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])]),\n",
    "    \n",
    "                    'test' : transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                            std=[0.229, 0.224, 0.225])])\n",
    "                 }\n",
    "\n",
    "#load image data set\n",
    "train_set = datasets.ImageFolder(data_train, transform = data_transform['train'])\n",
    "valid_set = datasets.ImageFolder(data_valid, transform = data_transform['valid'])\n",
    "test_set = datasets.ImageFolder(data_test, transform = data_transform['test'])\n",
    "\n",
    "#Dataloader\n",
    "batch_size = 15\n",
    "num_workers = 0\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size = batch_size,num_workers = num_workers,shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set,batch_size = batch_size,num_workers = num_workers,shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,batch_size = batch_size,num_workers = num_workers,shuffle = True)\n",
    "\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    'valid': valid_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "#check tensor size of the image\n",
    "sample = next(iter(train_set))\n",
    "image, label = sample\n",
    "print(image.shape)\n",
    "\n",
    "num_classes = train_set.classes\n",
    "print(len(num_classes))\n",
    "print(len(loaders['train']))\n",
    "print(len(loaders['valid']))\n",
    "print(len(loaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1152, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=133, bias=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#CNN model architecture\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #convo layers\n",
    "        self.conv1 = nn.Conv2d(3,32,3,2)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,2)\n",
    "        self.conv3 = nn.Conv2d(64,128,3,2)\n",
    "        #self.conv4 = nn.Conv2d(128,256,3)\n",
    "        #self.conv5 = nn.Conv2d(256,512,3)\n",
    "        \n",
    "        #pooling layer\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        #linear layers\n",
    "        self.fc1 = nn.Linear(128*3*3,2048)\n",
    "        self.fc2 = nn.Linear(2048,133)\n",
    "         \n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #first layer\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        #second layer\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        #third layer\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        '''\n",
    "        #fourth layer\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #fifth layer\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #x = self.dropout(x)\n",
    "        '''\n",
    "        \n",
    "        #reshape tensor\n",
    "        x = x.view(-1,128*3*3)\n",
    "        #last layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "         \n",
    "# instantiate the CNN\n",
    "net = Net()\n",
    "print(net)\n",
    "#print(net.conv1.weight.shape)\n",
    "#print(net.conv1.weight[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and validate model\n",
    "def train(n_epochs,model,loader,optimizer,criterion,save_path):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        \n",
    "        #training \n",
    "        net.train()\n",
    "        for batch, (data,target) in enumerate(loaders['train']):\n",
    "            \n",
    "            target = target.view(1,-1)\n",
    "            print(data.shape)\n",
    "            print(target.shape)\n",
    " \n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #get output\n",
    "            outputs = net(data)\n",
    "            print(outputs.shape)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = criterion(outputs,target)\n",
    "            \n",
    "            #backward prop\n",
    "            loss.backward()\n",
    "            \n",
    "            #update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            #calculate training loss\n",
    "            #train_loss += loss.item()\n",
    "            train_loss = train_loss + ((1 / (batch + 1)) * (loss.data - train_loss))\n",
    "\n",
    "            \n",
    "            #print results\n",
    "            if batch % 100 == 0:\n",
    "                print(\"Epoch: {}, Batch: {}, Training Loss: {}\".format(epoch+1, batch, train_loss))\n",
    "        \n",
    "        #validating\n",
    "        net.eval()\n",
    "        for batch, (data,target) in enumerate(loaders['valid']):\n",
    "            \n",
    "            #get output\n",
    "            outputs = net(data)\n",
    "            \n",
    "            #calculate loss\n",
    "            loss = criterion(outputs,target)\n",
    "\n",
    "            #calculate training loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            #print results\n",
    "            if batch % 100 == 0:\n",
    "                print(\"Epoch: {}, Batch: {}, Validation Loss: {}\".format(epoch+1, batch, valid_loss))\n",
    "            \n",
    "            '''       \n",
    "            if valid_loss < valid_loss_min:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "                valid_loss_min = valid_loss\n",
    "            ''' \n",
    "            \n",
    "            #save the model\n",
    "            torch.save(model.state_dict, save_path)\n",
    "            \n",
    "    #return net       \n",
    "    print(\"Finished Training and Validating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3, 224, 224])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([15, 133])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([1, 15])) that is different to the input size (torch.Size([15, 133])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (133) must match the size of tensor b (15) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da802bd0f980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'saved_model/dog_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-f91676128850>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, model, loader, optimizer, criterion, save_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#backward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (133) must match the size of tensor b (15) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "n_epochs = 1\n",
    "train(n_epochs,net,loaders,optimizer,criterion,'saved_model/dog_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "#net.load_state_dict(torch.load('saved_model/dog_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.745484\n",
      "\n",
      "\n",
      "Test Accuracy:  2% (18/836)\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "def test(loader, model, criterion):\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        #if use_cuda:\n",
    "         #   data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = net(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        #test loss\n",
    "        test_loss = test_loss + ((1 / (batch + 1)) * (loss.data - test_loss))\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders, net, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
